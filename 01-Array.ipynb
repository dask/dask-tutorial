{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/array.png\" width=\"25%\" align=\"right\">\n",
    "\n",
    "Arrays\n",
    "======\n",
    "\n",
    "Dask array provides a parallel, larger-than-memory, n-dimensional array using blocked algorithms.\n",
    "\n",
    "*  **Parallel**: Uses all of the cores on your computer\n",
    "*  **Larger-than-memory**:  Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "*  **Blocked Algorithms**:  Perform large computations by performing many smaller computations\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* http://dask.readthedocs.org/en/latest/array.html\n",
    "* http://dask.readthedocs.org/en/latest/array-api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blocked Algorithms\n",
    "---------------------\n",
    "\n",
    "A *blocked algorithm* executes on a large dataset by breaking it up into many small blocks.\n",
    "\n",
    "For example, consider taking the sum of a billion numbers.  We might instead break up the array into 1,000 chunks, each of size 1,000,000, take the sum of each chunk, and then take the sum of the intermediate sums.\n",
    "\n",
    "We acheive the intended result (one sum on one billion numbers) by performing many smaller results (one thousand sums on one million numbers each, followed by another sum of a thousand numbers.)\n",
    "\n",
    "We do exactly this with Python and NumPy in the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create data if it doesn't already exist\n",
    "from prep import random_array\n",
    "random_array()  \n",
    "\n",
    "# Load data with h5py\n",
    "import h5py\n",
    "import os\n",
    "f = h5py.File(os.path.join('data', 'random.hdf5'))\n",
    "dset = f['/x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute sum using blocked algorithm\n",
    "\n",
    "Here we compute the sum of this large array on disk by \n",
    "\n",
    "1.  Computing the sum of each 1,000,000 sized chunk of the array\n",
    "2.  Computing the sum of the 1,000 intermediate sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute sum of large array, one million numbers at a time\n",
    "sums = []\n",
    "for i in range(0, 1000000000, 1000000):\n",
    "    chunk = dset[i: i + 1000000]  # pull out numpy array\n",
    "    sums.append(chunk.sum())\n",
    "\n",
    "total = sum(sums)\n",
    "print total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:  Compute mean of positive elements using blocked algorithm\n",
    "\n",
    "Now that we've seen the simple example above try doing a more complicated problem, compute the mean of the positive elements.  You can do this by changing the code above with the following alterations:\n",
    "\n",
    "1.  Compute the number of elements above 1 in each 1,000,000 sized chunk of the array\n",
    "2.  Compute the sum of the 1,000 intermediate counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count number of elements greater than one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "%load solutions/Array-01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dask.array` contains these algorithms\n",
    "--------------------------------------------\n",
    "\n",
    "Dask.array is a NumPy-like library that does these kinds of tricks to operate on large datasets that don't fit into memory.  It extends beyond the linear problems discussed above to full N-Dimensional algorithms and a decent subset of the NumPy interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `dask.array` object\n",
    "\n",
    "You can create a `dask.array` `Array` object with the `da.from_array` function.  This function accepts\n",
    "\n",
    "1.  `data`: Any object that supports NumPy slicing, like `dset`\n",
    "2.  `chunks`: A chunk size to tell us how to block up our array, like `(1000000,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "x = da.from_array(dset, chunks=(1000000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulate `dask.array` object as you would a numpy array\n",
    "\n",
    "Now that we have an `Array` we perform standard numpy-style computations like arithmetic, mathematics, slicing, reductions, etc..\n",
    "\n",
    "The interface is familiar, but the actual work is different. dask_array.sum() does not do the same thing as numpy_array.sum().\n",
    "\n",
    "#### What's the difference?\n",
    "\n",
    "`dask_array.sum()` builds an expression of the computation. It does not do the computation yet. `numpy_array.sum()` computes the sum immediately.\n",
    "\n",
    "#### Why the difference?\n",
    "\n",
    "Dask arrays are split into chunks. Each chunk must have computations run on that chunk explicitly. If the desired answer comes from a small slice of the entire dataset, running the computation over all data would be wasteful of CPU and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = x.sum()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute result\n",
    "\n",
    "Dask.array objects are lazily evaluated.  Operations like `.sum` build up a graph of blocked tasks to execute.  \n",
    "\n",
    "We ask for the final result with a call to `.compute()`.  This triggers the actual computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:  Count of all elements greater than one\n",
    "\n",
    "Find out how many elements are greater than one using \n",
    "\n",
    "    (x > 1).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Does this match your result from before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance and Parallelism\n",
    "-------------------------------\n",
    "\n",
    "<img src=\"images/fail-case.gif\" width=\"40%\" align=\"right\">\n",
    "\n",
    "In our first examples we used `for` loops to walk through the array one block at a time.  For simple operations like `sum` this is optimal.  However for complex operations we may want to traverse through the array differently.  In particular we may want the following:\n",
    "\n",
    "1.  Use multiple cores in parallel\n",
    "2.  Chain operations on a single blocks before moving on to the next one\n",
    "\n",
    "Dask.array translates your array operations into a graph of inter-related tasks with data dependencies between them.  Dask then executes this graph in parallel with multiple threads.  We'll discuss more about this in the next section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "1.  Construct a 20000x20000 array of normally distributed random values broken up into 1000x1000 sized chunks\n",
    "2.  Take the mean along one axis\n",
    "3.  Take every 100th element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "x = da.random.normal(10, 0.1, size=(20000, 20000),   # 400 million element array \n",
    "                              chunks=(1000, 1000))   # Cut into 1000x1000 sized chunks\n",
    "y = x.mean(axis=0)[::100]                            # Perform NumPy-style operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.nbytes / 1e9  # Gigabytes of the input processed lazily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y.compute()     # Time to compute the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance comparision\n",
    "---------------------------\n",
    "\n",
    "The following experiment was performed on a heavy personal laptop.  Your performance may vary.  If you attempt the NumPy version then please ensure that you have more than 4GB of main memory.\n",
    "\n",
    "#### NumPy: 19s, Needs gigabytes of memory\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "%%time \n",
    "x = np.random.normal(10, 0.1, size=(20000, 20000)) \n",
    "y = x.mean(axis=0)[::100] \n",
    "y\n",
    "\n",
    "CPU times: user 19.6 s, sys: 160 ms, total: 19.8 s\n",
    "Wall time: 19.7 s\n",
    "```\n",
    "\n",
    "#### Dask Array: 4s, Needs megabytes of memory\n",
    "\n",
    "```python\n",
    "import dask.array as da\n",
    "\n",
    "%%time\n",
    "x = da.random.normal(10, 0.1, size=(20000, 20000), chunks=(1000, 1000))\n",
    "y = x.mean(axis=0)[::100] \n",
    "y.compute() \n",
    "\n",
    "CPU times: user 29.4 s, sys: 1.07 s, total: 30.5 s\n",
    "Wall time: 4.01 s\n",
    "```\n",
    "\n",
    "#### Discussion\n",
    "\n",
    "Notice that the dask array computation ran in 4 seconds, but used 29.4 seconds of user CPU time. The numpy computation ran in 19.7 seconds and used 19.6 seconds of user CPU time.\n",
    "\n",
    "Dask finished faster, but used more total CPU time because Dask was able to transparently parallelize the computation because of the chunk size.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "*  What happens if the dask chunks=(20000,20000)?\n",
    "    * Will the computation run in 4 seconds?\n",
    "    * How much memory will be used?\n",
    "* What happens if the dask chunks=(25,25)?\n",
    "    * What happens to CPU and memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:  Meteorological data\n",
    "\n",
    "There is 2GB of somewhat artifical weather data in HDF5 files in `data/weather-big/*.hdf5`.  We'll use the `h5py` library to interact with this data and `dask.array` to compute on it.\n",
    "\n",
    "Our goal is to visualize the average temperature on the surface of the Earth for this month.  This will require a mean over all of this data.  We'll do this in the following steps\n",
    "\n",
    "1.  Create `h5py.Dataset` objects for each of the days of data on disk (`dsets`)\n",
    "2.  Wrap these with `da.from_array` calls \n",
    "3.  Stack these datasets along time with a call to `da.stack`\n",
    "4.  Compute the mean along the newly stacked time axis with the `.mean()` method\n",
    "5.  Visualize the result with `matplotlib.pyplot.imshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from prep import create_weather  # Prep data if it doesn't exist\n",
    "create_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "filenames = sorted(glob(os.path.join('data', 'weather-big', '*.hdf5')))\n",
    "dsets = [h5py.File(filename)['/t2m'] for filename in filenames]\n",
    "dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsets[0][:5, :5]  # Slicing into h5py.Dataset object gives a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "plt.imshow(dsets[0][::4, ::4], cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrate with `dask.array`\n",
    "\n",
    "Make a list of `dask.array` objects out of your list of `h5py.Dataset` objects using the `da.from_array` function with a chunk size of `(500, 500)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack this list of `dask.array` objects into a single `dask.array` object with `da.stack`\n",
    "\n",
    "Stack these along the first axis so that the shape of the resulting array is `(31, 5760, 11520)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the mean of this array along the time (`0th`) axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "plt.imshow(..., cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%load solutions/Array-02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:  Subsample and store\n",
    "\n",
    "In the above exercise the result of our computation is small, so we can call `compute` safely.  Sometimes our result is still too large to fit into memory and we want to save it to disk.  In these cases you can use one of the following two functions\n",
    "\n",
    "1.  `da.store`: Store dask.array into any object that supports numpy setitem syntax, e.g.\n",
    "\n",
    "        f = h5py.File('myfile.hdf5')\n",
    "        output = f.create_dataset(shape=..., dtype=...)\n",
    "        \n",
    "        da.store(my_dask_array, output)\n",
    "        \n",
    "2.  `da.to_hdf5`: A specialized function that creates and stores a `dask.array` object into an `HDF5` file.\n",
    "\n",
    "        da.to_hdf5('data/myfile.hdf5', '/output', my_dask_array)\n",
    "        \n",
    "The task in this exercise is to use numpy step slicing to subsample the full dataset by a factor of two in both the latitude and longitude direction and then store this result to disk using one of the functions listed above.\n",
    "\n",
    "As a reminder, Python slicing takes three elements\n",
    "\n",
    "    start:stop:step\n",
    "\n",
    "    >>> L = [1, 2, 3, 4, 5, 6, 7]\n",
    "    >>> L[::3]\n",
    "    [1, 4, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "%load solutions/Array-03.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
