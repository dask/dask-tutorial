{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hdd.jpg\" width=\"20%\" align=\"right\">\n",
    "\n",
    "DataFrame Storage\n",
    "================\n",
    "\n",
    "Efficient storage can dramatically improve performance, particularly when operating repeatedly from disk.\n",
    "\n",
    "Decompressing text and parsing CSV files is expensive.  One of the most effective strategies with medium data is to use a binary storage format like HDF5.  Often the performance gains from doing this is sufficient so that you can switch back to using Pandas again instead of using `dask.dataframe`.\n",
    "\n",
    "In this section we'll learn how to efficiently arrange and store your datasets in on-disk binary formats.  We'll use the following:\n",
    "\n",
    "1.  [Pandas `HDFStore`](http://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5) format on top of `HDF5`\n",
    "2.  Categoricals for storing text data numerically\n",
    "3.  [Castra](http://github.com/Blosc/castra), an experimental data store optimized for `dask.dataframe`\n",
    "\n",
    "**Main Take-aways**\n",
    "\n",
    "1.  Storage formats affect performance by an order of magnitude\n",
    "2.  Text data will keep even a fast format like HDF5 slow\n",
    "3.  A combination of binary formats, column storage, and partitioned data turns one second wait times into 80ms wait times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note!\n",
    "\n",
    "If you're on a Mac you need to install the previous versions of `h5py` and `pytables`, like so:\n",
    "\n",
    "```sh\n",
    "conda install h5py=2.4.0 pytables=3.1.1\n",
    "```\n",
    "\n",
    "The most recent versions of these libraries cause a segfault when writing to HDF5 storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Create data if we don't have any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from prep import accounts_csvs\n",
    "accounts_csvs(3, 1000000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV\n",
    "\n",
    "First we read our csv data as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filename = os.path.join('data', 'accounts.*.csv')\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to HDF5\n",
    "\n",
    "Pandas contains a specialized HDF5 format, `HDFStore`.  The ``dd.DataFrame.to_hdf`` method works exactly like the ``pd.DataFrame.to_hdf`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = os.path.join('data', 'accounts.h5')\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df.to_hdf(target, '/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = dd.read_hdf(target, '/data')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare CSV to HDF5 speeds\n",
    "\n",
    "We do a simple computation that requires reading a column of our dataset and compare performance between CSV files and our newly created HDF5 file.  Which do you expect to be faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df2.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly they are about the same.  \n",
    "\n",
    "The culprit here is `names` column, which is of `object` dtype and thus hard to store efficiently.  There are two problems here:\n",
    "\n",
    "1.  How do we store text data like `names` efficiently on disk?\n",
    "2.  Why did we have to read the `names` column when all we wanted was `amount`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Store text efficiently with categoricals\n",
    "\n",
    "We can use Pandas categoricals to replace our object dtypes with a numerical representation.  This takes a bit more time up front, but results in better performance.\n",
    "\n",
    "More on categoricals at the [pandas docs](http://pandas-docs.github.io/pandas-docs-travis/categorical.html) and [this blogpost](http://matthewrocklin.com/blog/work/2015/06/18/Categoricals/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorize data, then store in HDFStore\n",
    "%time df.categorize(columns=['names']).to_hdf(target, '/data2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It looks the same\n",
    "df2 = dd.read_hdf(target, '/data2')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But loads more quickly\n",
    "%time df2.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is significantly faster.  This tells us that it's not only the file type that we use but also how we represent our variables that influences storage performance.\n",
    "\n",
    "However this can still be better.  We had to read all of the columns (`names` and `amount`) in order to compute the sum of one (`amount`).  We'll improve further on this with `castra`, an on-disk column-store.  First though we learn about how to set an index in a dask.dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Store columns separately with Castra\n",
    "\n",
    "[`Castra`](http://github.com/Blosc/castra) is an on-disk column-store that is partitioned along the index.  It matches the dask.dataframe model exactly.\n",
    "\n",
    "You will likely have to install castra using `pip`.\n",
    "\n",
    "    pip install castra\n",
    "    \n",
    "*Disclaimer: Castra is experimental and under heavy development churn.  You should not use it in production.*\n",
    "\n",
    "*We discuss Castra here to demonstrate the value of thinking hard about storage, not as an endorsement of its use.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if os.path.exists('accounts.castra'):\n",
    "    import shutil\n",
    "    shutil.rmtree('accounts.castra')\n",
    "\n",
    "c = df.to_castra('accounts.castra', categories=['names'])\n",
    "df3 = c.to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df3.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time df3.names.drop_duplicates().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Castra and partitioned indexes\n",
    "\n",
    "As discussed in the [previous notebook](03a-DataFrame.ipynb), `dask.dataframe` partitions your data along the index.  This can make some queries, like `loc`, `groupby`, and `join/merge` much faster because `dask.dataframe` can selectively load data and because it knows that related data is close-by.\n",
    "\n",
    "Castra uses the same model of data partitioned along the index when it stores data on disk.  By using the two together we can get efficient disk-based queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Index your dataframe by `id`, then store it into a `Castra` file as we do above.  Inspect your data and verify that the index column is `id` and that it starts at `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the divisions of your dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.loc` accessor to get the records corresponding to account `100`.  Compare this to the speed you saw in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Storage choices strongly impact performance.  We evolved from text-based CSV files to binary-based Castra and saw our query times drop from 1s to 80ms.\n",
    "\n",
    "We also used `DataFrame.set_index` to organize our data along a special column.  A common recipe for success with `dask.dataframe` is as follows:\n",
    "\n",
    "1.  Read in your data however it was delivered to you\n",
    "\n",
    "        df = dd.read_csv('myfiles.*.csv')\n",
    "    \n",
    "2.  Set your index \n",
    "\n",
    "        df2 = df.set_index('column-name')\n",
    "        \n",
    "3.  Base computation on Castra file\n",
    "\n",
    "        c = df2.to_castra('/path/to/new/file.castra', \n",
    "                          categories=['list', 'of', 'columns', 'to', 'categorize'])\n",
    "        df3 = c.to_dask()\n",
    "        \n",
    "4.  Perform efficient queries\n",
    "\n",
    "        df3.loc['2014': '2015'].groupby().events.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
